= Differentiability =
:doctype: book
:author: Bharath Kumar S P
:email: bharath030195@gmail.com
:stem: latexmath
:eqnums:
:toc:

== Single Variable Function ==
Let stem:[f: \mathbb{R} \rightarrow \mathbb{R}]. The function stem:[f] is differentiable at stem:[x_0] iff the limit

[stem]
++++
\lim_{h \to 0} \frac{f(x_0+h) - f(x_0)}{h}
++++

exists. And this limit value is itself called as the derivative of the function at stem:[x_0].

*Alternate Definition:*

We can approximate a function linearly at a point using the notion of differentiability. Does there exist a good linear approximation of the function stem:[f] at stem:[x_0]? Such a linear function should

* Exactly equal stem:[f(x_0)] at stem:[x_0].
* The error we make by approximating the original function by this linear function should decay to 0 if we move towards stem:[x_0]. The error should be quadratic to the distance stem:[|x-x_0|], i.e., the error should be stem:[|x-x_0|^2]. The error should decay to 0 at least as fast as the distance stem:[|x-x_0|] going to 0. We say the linear approximation error should be quadratic, the quadratic approximation error should be cubic, etc.

Say we are at stem:[x_0], the function value is stem:[f(x_0)]. A line passing the point stem:[(x_0, y_0)] can be written as stem:[y-y_0 = m(x-x_0)]. In our case, this becomes stem:[f(x) - f(x_0) = m(x-x_0)].

image::.\images\derivatives_01.png[align='center', 400, 300]

Then the error in approximating the original function by this linear function is given by stem:[f(x) - (f(x_0) + m(x-x_0))]. Then this error per unit change in the distance should go to 0 as fast as stem:[x] going to stem:[x_0].

[stem]
++++
\lim_{x \to x_0} \frac{|f(x) - (f(x_0) + m(x-x_0))|}{|x-x_0|} = 0
++++

There should exists an stem:[m] such that the above limit goes to 0. Then we say the function is differentiable at stem:[x_0]. This stem:[m] is the value of the derivative of stem:[f] at stem:[x_0].

== Multivariate Function ==
Let stem:[f: \mathbb{R}^n \rightarrow \mathbb{R}]. Suppose stem:[f(\mathbf{x}) = x_1^2 + x_2^2]. Say we want to determine the differentiability of the function at stem:[(\bar{x}_1, \bar{x}_2)]. There are countably infinite direction in which we can approach stem:[(\bar{x}_1, \bar{x}_2)]. 

Say we are at stem:[\mathbf{x}_0 \in \mathbb{R}^n] and we are given a direction stem:[\mathbf{u} \in \mathbb{R}^n]. Let's restrict the movement along the stem:[\mathbf{u}] direction. We end up considering this restricted function stem:[f(\mathbf{x}_0 + h \mathbf{u})] where stem:[h] is a scalar, and this is a function of a single variable stem:[h].

[stem]
++++
g(h) = f(\mathbf{x}_0 + h \mathbf{u})
++++

We can find the derivative of this single variable function, stem:[g'(h)]. This stem:[g'(h)] is called as directional derivative. We get one directional derivative for every stem:[\mathbf{u}].

====
The instantaneous rate of change of stem:[f(\mathbf{x})] at stem:[\mathbf{x}_0] in the direction of stem:[\mathbf{u}] is called the directional derivative and is denoted by stem:[D_{\mathbf{u}} f(\mathbf{x}_0)]

[stem]
++++
D_{\mathbf{u}} f(\mathbf{x}_0) \equiv \lim_{h \to 0} \frac{f(\mathbf{x}_0 + h \mathbf{u}) - f(\mathbf{x}_0)}{h}
++++
====

When we have a vector space, we can express any vector as a linear combination of the basis vectors. So there are some fundamental directions. Say stem:[f: \mathbb{R}^3 \rightarrow \mathbb{R}] and stem:[\mathbf{u} = (1,0,0), (0,1,0), (0,0,1)]. Then 

[stem]
++++
\begin{align*}
g_1(h) & = f(\mathbf{x}_0 + h (1,0,0)) \\
g_2(h) & = f(\mathbf{x}_0 + h (0,1,0)) \\
g_3(h) & = f(\mathbf{x}_0 + h (0,0,1)) \\
\end{align*}
++++

Derivatives of these fundamental directions' single variable functions are called as partial derivatives of stem:[f] at stem:[\mathbf{x_0}]. Suppose stem:[f(\mathbf{x}) = x_1^2 + x_2^2], then the partial derivatives are given by

[stem]
++++
\begin{align*}
\frac{\partial f}{\partial x_1} & = 2x_1 \\
\frac{\partial f}{\partial x_2} & = 2x_2 \\
\end{align*}
++++

How can we define the differentiability of the function at stem:[\mathbf{x_0}]? A general linear function (a hyperplane) defined in stem:[\mathbb{R}^n] passing through a point stem:[(\mathbf{x}_0, f(\mathbf{x}_0))] is given by

[stem]
++++
l_{\mathbf{x}_0}(\mathbf{x}) = f(\mathbf{x}_0) + \mathbf{w}^\top (\mathbf{x}-\mathbf{x}_0)
++++

This gives us the general linear approximation of stem:[f] at stem:[\mathbf{x}_0]. So if there exists a stem:[\mathbf{w} \in \mathbb{R}^n] such that 

[stem]
++++
\lim_{\mathbf{x} \to \mathbf{x}_0} \frac{|f(\mathbf{x}) - (f(\mathbf{x}_0) + \mathbf{w}^\top (\mathbf{x}-\mathbf{x}_0))|}{\| \mathbf{x} - \mathbf{x}_0\|} = 0
++++

the limit goes to 0, then we say the function is differentiable at stem:[\mathbf{x}_0]. And we call such stem:[\mathbf{w}] as gradient of stem:[f] which is evaluated at stem:[\mathbf{x}_0]. The gradient of stem:[f] at stem:[\mathbf{x}_0] is denoted by stem:[\nabla f(\mathbf{x}_0)]. The gradient will be in the same space as the input vector.

====
*Theorem:*

If stem:[f] is differentiable at stem:[\mathbf{x}_0], then stem:[D_{\mathbf{u}} f(\mathbf{x}_0) = \langle \nabla f(\mathbf{x}_0), \mathbf{u} \rangle]
====

This theorem helps us compute the gradient. When stem:[\mathbf{u}=(1,0,0)], the LHS gives the first partial derivative and the RHS gives the first entry of stem:[\nabla f(\mathbf{x}_0)]. Thus the first entry in the gradient vector is the first partial derivative, the second entry is the second partial derivative, etc.

*Example 01:*

Let stem:[f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + b]. Find the gradient of stem:[f].

We know stem:[f(\mathbf{x}) = \sum_{i=1}^n w_i x_i + b]. Then 

[stem]
++++
\frac{\partial f}{\partial x_1} = w_1; \hspace{1cm}\dots\hspace{1cm} ;\frac{\partial f}{\partial x_n} = w_n 
++++

Hence the gradient of stem:[f] is stem:[\nabla f(\mathbf{x}) = \mathbf{w}], which is a constant vector. For scalar-valued linear functions, the gradient is a constant.

*Example 02:*

Let stem:[f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top \mathbf{P} \mathbf{x} + \mathbf{q}^\top \mathbf{x} + r]. Find the gradient of stem:[f]. Assume the matrix stem:[\mathbf{P}] is symmetric.

As we are taking derivatives, we can take the derivative term by term and then add them up. The gradient of stem:[r] is stem:[\mathbf{0}]. The gradient of stem:[\mathbf{q}^\top \mathbf{x}] is stem:[\mathbf{q}]. Let stem:[g(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top \mathbf{P} \mathbf{x} = \frac{1}{2} \sum_{i,j} x_i x_j P_{ij}].

[stem]
++++
\begin{align*}
g(\mathbf{x}) & = \frac{1}{2} \mathbf{x}^\top \mathbf{P} \mathbf{x} \\
& = \frac{1}{2} P_{11} x_1^2 + x_1 x_2 P_{12} + x_1 x_3 P_{13} + \dots + x_1 x_n P_{1n} + \dots
\end{align*}
++++

NOTE: stem:[x_1 x_2 P_{12} = x_1 x_2 P_{21}], so stem:[\frac{1}{2} * 2 x_1 x_2 P_{12} = x_1 x_2 P_{12}].

[stem]
++++
\begin{align*}
\frac{\partial g(\mathbf{x})}{\partial x_1} & = P_{11} x_1 + P_{12} x_2 + \dots + P_{1n} x_n = \mathbf{Px}\,|_1 \\
\frac{\partial g(\mathbf{x})}{\partial x_2} & = \mathbf{Px}\,|_2 \\
\dots \\
\frac{\partial g(\mathbf{x})}{\partial x_n} & = \mathbf{Px}\,|_n \\
\end{align*}
++++

Note stem:[\mathbf{Px}] is a vector and each partial derivative is an entry of the vector. Stacking them all gives stem:[\nabla g(\mathbf{x}) = \mathbf{Px}]. Then stem:[\nabla f(\mathbf{x}) = \mathbf{Px} + \mathbf{q}].

*Example 03:*

Let stem:[f(\mathbf{x}) = \sum_{i=1}^n x_i \log x_i], where stem:[x_i >0]. Then stem:[\nabla f(\mathbf{x}) = 1 + \log \mathbf{x}] where stem:[\log \mathbf{x}] is the entry-wise log.

*Example 04:*

Let stem:[f(\mathbf{X}) = tr(\mathbf{X})]. This function takes a square matrix and gives a scalar. Find the gradient of stem:[f]. The domain of the function is the vector space of matrices. The inner product defined in a matrix vector space is the Frobenius inner product.

The general form of a linear function in the matrix vector space passing through stem:[(\mathbf{X}_0, f(\mathbf{X}_0))] is given by

[stem]
++++
l_{\mathbf{X}_0}(\mathbf{X}) = f(\mathbf{X}_0) + \langle \nabla f(\mathbf{X}_0), \mathbf{X}- \mathbf{X}_0 \rangle_F
++++

Here stem:[\nabla f(\mathbf{X}_0)] is a matrix. If there exists a matrix stem:[\nabla f(\mathbf{X}_0)] such that

[stem]
++++
\lim_{\mathbf{X} \to \mathbf{X}_0 } \frac{|f(\mathbf{X}) - l_{\mathbf{X}_0}(\mathbf{X})|}{\| \mathbf{X} - \mathbf{X}_0 \|_F} = 0
++++

the limit goes to 0, then we say the function is differentiable at stem:[\mathbf{X}_0]. And such a matrix is the gradient of stem:[f] at stem:[\mathbf{X}_0]. To compute this gradient, take the fundamental basis of matrices. Then by the theorem

[stem]
++++
\left \langle \nabla f(\mathbf{X}_0), \begin{bmatrix} 1 & \dots & 0 \\ \vdots & \dots & \vdots  \\ 0  & \dots & 0 \end{bmatrix} \right \rangle = D_{\mathbf{U}}f(\mathbf{X}_0)
++++

The stem:[a_{11}] entry of the matrix stem:[\nabla f(\mathbf{X}_0)] is the first directional derivative, i.e., the partial derivative of the function with respect to the first variable. Similarly, the partial derivative with the second variable gives the stem:[a_{12}] entry of the matrix stem:[\nabla f(\mathbf{X}_0)], and so on. stem:[\mathbf{X}] has stem:[n \times n] variables.

[stem]
++++
\frac{\partial f(\mathbf{X})}{\partial X_{ij}} = \frac{\partial tr(\mathbf{X})}{\partial X_{ij}} = \frac{\partial (\sum_k X_{kk})}{\partial X_{ij}} = \mathbf{I}
++++

So the gradient of the function is the identity matrix.

*Example 05:*

Let stem:[f(\mathbf{X}) = \langle \mathbf{A}, \mathbf{X} \rangle_F]. Then stem:[\nabla f(\mathbf{X}) = \mathbf{A}].

== Summary ==

* When stem:[f: \mathbb{R} \rightarrow \mathbb{R}], then the linear approximation of stem:[f] at stem:[x_0] is stem:[l_{x_0}(x) = f(x_0) + m (x-x_0)], where stem:[m] is the derivative of stem:[f] at stem:[x_0].

* When stem:[f: \mathbb{R}^n \rightarrow \mathbb{R}], then the linear approximation of stem:[f] at stem:[\mathbf{x}_0] is stem:[l_{\mathbf{x}_0}(\mathbf{x}) = f(\mathbf{x}_0) + \mathbf{w}^\top (\mathbf{x} - \mathbf{x}_0)], where stem:[\mathbf{w}] is the gradient of stem:[f] at stem:[\mathbf{x}_0].

* When stem:[f: \mathbb{R}^n \rightarrow \mathbb{R}^m] (a vector-valued function), then the linear approximation of stem:[f] at stem:[\mathbf{x}_0] is stem:[l_{\mathbf{x}_0}(\mathbf{x}) = f(\mathbf{x}_0) + \mathbf{A} (\mathbf{x} - \mathbf{x}_0)]. If there exists a stem:[\mathbf{A} \in \mathbb{R}^{m \times n}] such that
+
[stem]
++++
\lim_{\mathbf{x} \to \mathbf{x}_0} \frac{\| f(\mathbf{x}) - (f(\mathbf{x}_0) + \mathbf{A} (\mathbf{x} - \mathbf{x}_0))\|}{\| \mathbf{x} - \mathbf{x}_0\|} = 0
++++
+
the limit goes to 0, then we say the function is differentiable at stem:[\mathbf{x}_0]. And we call such stem:[\mathbf{A}] as the Jacobian of stem:[f] which is evaluated at stem:[\mathbf{x}_0]. And it can be shown that the entries of the Jacobian matrix are
+
[stem]
++++
Df(\mathbf{x}) = \nabla f(\mathbf{x}) =
\begin{bmatrix}
\frac{\partial f_1(\mathbf{x}) }{\partial x_1} & \dots & \frac{\partial f_1(\mathbf{x}) }{\partial x_n} \\
\vdots & \dots & \vdots \\
\frac{\partial f_m(\mathbf{x}) }{\partial x_1} & \dots & \frac{\partial f_m(\mathbf{x}) }{\partial x_n} \\
\end{bmatrix}
++++