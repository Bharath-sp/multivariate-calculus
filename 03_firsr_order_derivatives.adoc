= First-Order Derivatives =
:doctype: book
:stem: latexmath
:eqnums:
:toc:

In single-variable calculus, the derivative tells us how a function changes at a point - how steep the hill is under your feet. But as we step into higher dimensions, the concept evolves dramatically. Suddenly, we're not just talking about slopes, but about gradients that point in the direction of greatest change, Jacobians that capture how systems distort space.

== Review of Linear Maps ==
The idea of linear maps helps us understand the (first-order) derivatives much better, especially in high dimensional spaces. So, a quick recap of this is as follows.

We know that a vector-valued linear function stem:[f: \mathbb{R}^n \to \mathbb{R}^m] can be represented by stem:[f(\mathbf{x}) = \mathbf{Ax}], where stem:[\mathbf{A}] is an stem:[m \times n] matrix. In linear algebra, we look at this as a linear mapping from the vector space stem:[\mathbb{R}^n] to the vector space stem:[\mathbb{R}^m]. When we fix the standard basis for stem:[\mathbb{R}^n] and stem:[\mathbb{R}^m], every linear map stem:[f: \mathbb{R}^n \to \mathbb{R}^m] corresponds to a *unique* matrix stem:[\mathbf{A} \in \mathbb{R}^{m \times n}] such that

[stem, id='eq_1']
++++
\begin{align}
f(\mathbf{x}) = \mathbf{Ax}
\end{align}
++++

That is, once the basis is fixed, the matrix fully determines the linear map. So, with a bit of abuse of language, it is common to say "Let stem:[\mathbf{A} \in \mathbb{R}^{m \times n}] be a linear map". But what it really means is: "Let stem:[f(\mathbf{x}) = \mathbf{Ax}] be a linear map".

== Derivative of Single Variable Functions ==
Let stem:[f: \mathbb{R} \to \mathbb{R}]. For stem:[h>0], the derivative of stem:[f] with respect to stem:[x] at stem:[x_0], denoted by stem:[f'(x_0)] or stem:[\frac{d f}{d x}(x_0)], is defined as

[stem, id='eq_2']
++++
\begin{align}
f'(x_0) = \frac{d f}{d x} (x_0) := \lim_{h \to 0} \frac{f(x_0+h) - f(x_0)}{h}
\end{align}
++++

provided the limit exists. The function stem:[f] is differentiable at stem:[x_0] iff this limit exists. And this limit value is itself called as the derivative of the function at stem:[x_0]. This measures the instantaneous rate at which the function stem:[f] changes for a inifinitesimal small change from stem:[x_0]. stem:[f(x)] is called differentiable on an interval if the derivative exists for every point in that interval.

*Alternate Definition:*

Let stem:[f: \mathbb{R} \to \mathbb{R}] be any function. We can approximate this function at a point stem:[x_0] using a linear function. Such a linear function should

* Exactly equal stem:[f(x_0)] at stem:[x_0].
* The error we make by approximating the original function by this linear function should decay to 0 as we move towards stem:[x_0]. The error should decay to 0 at least as fast as the distance stem:[|x-x_0|] decay to 0. That is, the error should at least be quadratic to the distance stem:[|x-x_0|], i.e., it should be stem:[|x-x_0|^2]. In general, the linear approximation error should be quadratic, the quadratic approximation error should be cubic, etc.

Say we are at stem:[x_0], the function value is stem:[f(x_0)]. A line passing the point stem:[(x_0, f(x_0))] can be written as stem:[f(x) - f(x_0) = m(x-x_0)].

image::.\images\derivatives_01.png[align='center', 300, 300]

The linear (affine) function

[stem]
++++
l(x) = f(x_0) + m(x-x_0)
++++

is called the first-order approximation of stem:[f] at (or near) stem:[x_0]. The error in approximating the original function by this linear function is given by stem:[f(x) - (f(x_0) + m(x-x_0))]. If there exists an stem:[m] such that

[stem, id='eq_3']
++++
\begin{align}
\lim_{x \to x_0} \frac{|f(x) - (f(x_0) + m(x-x_0))|}{|x-x_0|} = 0
\end{align}
++++

goes to 0, then we say the function is differentiable at stem:[x_0]. And we call this stem:[m] as the derivative of stem:[f] at stem:[x_0]. Then we can write

[stem]
++++
\begin{align*}
f(x) & \approx  f(x_0) + f'(x_0) (x-x_0) \\
f(x) - f(x_0) & \approx  f'(x_0) (x-x_0) \\
\Delta f & \approx f'(x_0) \Delta x
\end{align*}
++++

For example, let the function be stem:[f(x)=x^2] with stem:[(x_0,f(x_0)) = (3,9)] and we know that stem:[f'(x_0) = 6].

[stem]
++++
\begin{align*}
f(3.0001) & = 9.00060001 \\
f(3.00001) & = 9.0000600001 \\
f(3.000001) & = 9.000006000001 \\
f(3.0000001) & = 9.00000060000001 \\
f(3 + \Delta x) & \approx 9 + 6 \Delta x \\
f(x) & \approx f(3) + 6 (x-3)
\end{align*}
++++

This is the linear approximation of stem:[x^2] at stem:[x=3]. We can see that, as we get closer to 3, the RHS gets closer to 9. On writing it as:

[stem]
++++
\begin{align*}
f(x)- f(3) & \approx 6 (x-3) \\
\Delta f & \approx f'(x_0) \Delta x
\end{align*}
++++

So, to know the change in stem:[f], we can just multiply the change in stem:[x] by stem:[f'(x_0)]. This becomes:

[stem]
++++
df = f'(x_0) dx
++++

as we take the infinitesimal limit (as the change in stem:[x] tends to 0). Comparing this form with <<eq_1, Equation 1>> helps us see that "the derivative of a function at a point is the unique linear map that best approximates the change in the function's value near that point".

====
The derivative at stem:[x_0] is commonly interpreted (especially in high school calculus) as the slope of the tangent line at stem:[x_0], or as the instantaneous rate of change of the function at stem:[x_0]

[stem]
++++
\frac{df}{dx}(x_0) = f'(x_0)
++++

However, a more rigorous and modern view defines the derivative as a linear map.

The derivative at stem:[x_0], viewed as a linear map, takes a small input increment stem:[dx \in \mathbb{R}] and maps it to stem:[f'(x_0) \cdot dx], which is the best linear approximation to the change in the function value stem:[f(x_0+dx) - f(x_0)].

[stem]
++++
df = f'(x_0) \cdot dx
++++
====

So, to find the derivative of stem:[f] at stem:[x_0], instead of solving <<eq_2, Equation 2>>, we can derive the first-order approximation of stem:[f] at stem:[x_0] and find an stem:[m] that satisfies <<eq_3, Equation 3>>.

== Derivative of Multivariable Functions ==
Let stem:[f: \mathbb{R}^n \rightarrow \mathbb{R}] be a multivariable scalar-valued function. Similar to the single-variable case, the derivative of a multivariable function is fundamentally about measuring the rate at which the function changes. However, with multiple independent variables, the notion of the "rate of change" becomes more nuanced as we can consider changes with respect to each variable or along any specific direction among the countably infinite directions.

Thus, there are different forms of derivatives.

=== Partial Derivatives ===
The partial derivatives measure the rate of change of a multivariable function with respect to one variable, while holding all other variables constant. For a function stem:[f(x_1, x_2)], keeping stem:[x_2] fixed, we may want to know how stem:[f] changes as stem:[x_1] changes: stem:[\frac{\partial f}{\partial x_1}]. This is defined as

[stem]
++++
\frac{\partial f}{\partial x_1} := \lim_{h \to 0} \frac{f(x_1 +h, x_2) - f(x_1, x_2)}{h}
++++

=== Directional Derivatives ===
The directional derivatives generalize the partial derivative to measure the rate of change of a function along an arbitrary direction (given by a unit vector stem:[\mathbf{u}]). Say we are at stem:[\mathbf{x}_0 \in \mathbb{R}^n] and we are given a direction stem:[\mathbf{u} \in \mathbb{R}^n]. On restricting the movement along the stem:[\mathbf{u}] direction, we end up with this restricted function stem:[f(\mathbf{x}_0 + h \mathbf{u})] where stem:[h] is a scalar, and this is a function of a single variable stem:[h].

[stem]
++++
g(h) = f(\mathbf{x}_0 + h \mathbf{u})
++++

We can find the derivative of this single variable function, stem:[g'(h)]. This stem:[g'(h)] is called as the directional derivative of stem:[f] at stem:[x_0] in the direction stem:[\mathbf{u}]. We get one directional derivative for every direction stem:[\mathbf{u}].

Formally, the instantaneous rate of change of stem:[f(\mathbf{x})] at stem:[\mathbf{x}_0] in the direction stem:[\mathbf{u}] is called the directional derivative and is denoted by stem:[D_{\mathbf{u}} f(\mathbf{x}_0)]

[stem]
++++
D_{\mathbf{u}} f(\mathbf{x}_0) \equiv \lim_{h \to 0} \frac{f(\mathbf{x}_0 + h \mathbf{u}) - f(\mathbf{x}_0)}{h}
++++

The directional derivative in the standard directions of the vector space gives us the partial derivatives. We know that in a vector space, we can express any vector as a linear combination of the basis vectors. These basis vectors form the standard directions. For example, for the function stem:[f: \mathbb{R}^2 \rightarrow \mathbb{R}], the standard directions are stem:[\mathbf{u} = (1,0)] and stem:[(0,1)]. Then 

[stem]
++++
\begin{align*}
g_1(h) & = f(\mathbf{x}_0 + h (1,0)) \\
g_2(h) & = f(\mathbf{x}_0 + h (0,1)) \\
\end{align*}
++++

Derivatives of these single variable functions are the partial derivatives of stem:[f] at stem:[\mathbf{x_0}]. Suppose stem:[f(\mathbf{x}) = x_1^2 + x_2^2], then

[stem]
++++
\begin{align*}
g_1(h) & = f((x_1, x_2) + h (1,0)) = f(x_1+h, x_2) \\
g_2(h) & = f((x_1, x_2) + h (0,1)) = f(x_1, x_2+h) \\
\end{align*}
++++

The derivative of these functions give us stem:[\frac{\partial f}{\partial x_1}] and stem:[\frac{\partial f}{\partial x_2}] at stem:[(x_1,x_2)]. And we know that,

[stem]
++++
\frac{\partial f}{\partial x_1} \Bigg|_{(x_1, x_2)} = 2x_1; \hspace{1cm} \frac{\partial f}{\partial x_2} \Bigg|_{(x_1, x_2)} = 2x_2 
++++

=== Gradient ===
The gradient of a scalar-valued multivariable function at a point stem:[\mathbf{x}_0], often denoted by stem:[\nabla f(\mathbf{x}_0)], is a vector whose components are the partial derivatives of the function at stem:[\mathbf{x}_0]. For stem:[f(x_1, x_2)], the gradient at stem:[(x_1, x_2)] is

[stem]
++++
\nabla f(x_1,x_2) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \Bigg|_{(x_1, x_2)} \\ \frac{\partial f}{\partial x_2} \Bigg|_{(x_1, x_2)} \end{bmatrix}
++++

The gradient is always a column vector and this vector points in the direction of the greatest rate of increase of the function. For every scalar-valued multivariable function, the gradient will always have the same shape as the input. So, it will be in the same space as the input vector.

=== Total Derivative ===
Let stem:[f: \mathbb{R}^n \rightarrow \mathbb{R}]. How can we define the differentiability of the function at a point stem:[\mathbf{x_0}]? We can follow the same method that we did for the single variable case. The linear (affine) function

[stem]
++++
l(\mathbf{x}) = f(\mathbf{x}_0) + \mathbf{w}^\top (\mathbf{x}-\mathbf{x}_0)
++++

is called the first-order approximation of stem:[f] at (or near) stem:[\mathbf{x}_0]. The error in approximating the original function by this linear function is given by stem:[f(\mathbf{x}) - (f(\mathbf{x}_0) + \mathbf{w}^\top (\mathbf{x}-\mathbf{x}_0))]. If there exists a stem:[\mathbf{w} \in \mathbb{R}^n] such that 

[stem, id='eq_4']
++++
\begin{align}
\lim_{\mathbf{x} \to \mathbf{x}_0} \frac{|f(\mathbf{x}) - (f(\mathbf{x}_0) + \mathbf{w}^\top (\mathbf{x}-\mathbf{x}_0))|}{\| \mathbf{x} - \mathbf{x}_0\|_2} = 0
\end{align}
++++

goes to 0, then we say the function is differentiable at stem:[\mathbf{x}_0]. And we call such stem:[\mathbf{w}^\top] as the (total) derivative of stem:[f] at stem:[\mathbf{x}_0], often denoted by stem:[D f(\mathbf{x}_0)]. There can be at most one stem:[\mathbf{w}] that satisfies this.

[stem]
++++
D f(\mathbf{x}_0) = \mathbf{w}^\top
++++

====
*Theorem:*

If stem:[f] is differentiable at stem:[\mathbf{x}_0], then the directional derivative stem:[D_{\mathbf{u}} f(\mathbf{x}_0) = \langle \mathbf{w}, \mathbf{u} \rangle]
====

This theorem helps us find stem:[\mathbf{w}]. When stem:[\mathbf{u}=(1,0)], the LHS gives the first partial derivative and the RHS gives the first entry of stem:[\mathbf{w}]. So, the first entry of stem:[\mathbf{w}] should be the first partial derivative, the second entry should be the second partial derivative, etc. stem:[\mathbf{w}] should be the gradient of stem:[f] at stem:[\mathbf{x}_0]. Hence, the derivative of stem:[f] at stem:[\mathbf{x}_0] is

[stem]
++++
D f(\mathbf{x}_0) = \nabla f(\mathbf{x}_0)^\top = \begin{bmatrix} \frac{\partial f}{\partial x_1} & \dots & \frac{\partial f}{\partial x_n} \end{bmatrix}
++++

Then we can write

[stem]
++++
\begin{align*}
f(\mathbf{x}) & \approx f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^\top (\mathbf{x}-\mathbf{x}_0) \\
f(\mathbf{x}) - f(\mathbf{x}_0) & \approx  \nabla f(\mathbf{x}_0)^\top (\mathbf{x}-\mathbf{x}_0) \\
\Delta f & \approx  \nabla f(\mathbf{x}_0)^\top \Delta \mathbf{x} \\
\end{align*}
++++

So, to know the change in stem:[f], we can just take the dot product of the stem:[\Delta \mathbf{x}] vector with the vector stem:[\nabla f(\mathbf{x}_0)^\top]. This becomes:

[stem]
++++
df = \nabla f(\mathbf{x}_0)^\top d\mathbf{x}
++++

as we take the infinitesimal limit (as the change in stem:[\mathbf{x}] tends to 0). Comparing this form with <<eq_1, Equation 1>> helps us see that "the derivative of stem:[f] at the point stem:[\mathbf{x}_0], stem:[\nabla f(\mathbf{x}_0)^\top], is a linear map that best approximates the change in stem:[f] near stem:[\mathbf{x}_0]." This linear form tells us the first-order approximation of how the function changes in response to a small change in stem:[\mathbf{x}].

For scalar-valued multivariable functions, the derivative stem:[D f(\mathbf{x}_0)] is a row vector stem:[\nabla f(\mathbf{x}_0)^\top \in \mathbb{R}^{1 \times n}], which is the transpose of the gradient vector.

====
The derivative of a multivariable scalar function stem:[f] at stem:[\mathbf{x}_0] can be found by the following two ways:

* From the partial derivatives stem:[\frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n}]: this is a tedious task as stem:[n] becomes large.

* By deriving the first-order approximation of stem:[f] at stem:[\mathbf{x}_0], i.e., the vector stem:[D f(\mathbf{x}_0)] that satisfies <<eq_4, Equation 4>>.
====

== Derivative of Vector-Valued Functions ==
Let stem:[f: \mathbb{R}^n \rightarrow \mathbb{R}^m] be a vector-valued function. The function stem:[f] is differentiable at stem:[\mathbf{x}_0] if there exists a matrix stem:[Df(\mathbf{x}_0) \in \mathbb{R}^{m \times n}] that satisfies

[stem, id='eq_5']
++++
\begin{align}
\lim_{\mathbf{x} \to \mathbf{x}_0} \frac{\|f(\mathbf{x}) - (f(\mathbf{x}_0) + Df(\mathbf{x}_0) (\mathbf{x}-\mathbf{x}_0))\|_2}{\| \mathbf{x} - \mathbf{x}_0\|_2} = 0
\end{align}
++++

in which case we refer to stem:[Df(\mathbf{x}_0)] as the derivative (or Jacobian) of stem:[f] at stem:[\mathbf{x}_0]. There can be at most one matrix that satisfies this. The linear (affine) function 

[stem]
++++
l(\mathbf{x}) = f(\mathbf{x}_0) + Df(\mathbf{x}_0) (\mathbf{x}-\mathbf{x}_0)
++++

is called the first-order approximation of stem:[f] at (or near) stem:[\mathbf{x}_0]. Then


[stem]
++++
\begin{align*}
f(\mathbf{x}) - f(\mathbf{x}_0) & \approx  D f(\mathbf{x}_0) (\mathbf{x}-\mathbf{x}_0) \\
\Delta f & \approx  D f(\mathbf{x}_0) \Delta \mathbf{x} \\
\end{align*}
++++

So, to know the change in stem:[f], we can just do matrix-vector multiplication of stem:[\Delta \mathbf{x}] and stem:[D f(\mathbf{x}_0)]. This becomes:

[stem]
++++
df = D f(\mathbf{x}_0) d\mathbf{x}
++++

as we take the infinitesimal limit (as the change in stem:[\mathbf{x}] tends to 0). Comparing this form with <<eq_1, Equation 1>> helps us see that "the derivative of stem:[f] at the point stem:[\mathbf{x}_0], stem:[D f(\mathbf{x}_0)], is a linear map that best approximates the change in stem:[f] near stem:[\mathbf{x}_0]."

For vector-valued functions, the derivative stem:[D f(\mathbf{x}_0)] is a matrix stem:[\mathbb{R}^{m \times n}], which is known as the Jacobian.

====
The derivative of a vector-valued function stem:[f] at stem:[\mathbf{x}_0] can be found by the following two ways:

* From the partial derivatives stem:[Df(\mathbf{x}_0)_{ij} = \frac{\partial f_i}{\partial x_j}, \,\, \, i=1, \dots, m, \,\, j=1,\dots,n]: this is obviously a tedious task.

* By deriving the first-order approximation of stem:[f] at stem:[\mathbf{x}_0], i.e., the matrix stem:[D f(\mathbf{x}_0)] that satisfies <<eq_5, Equation 5>>.
====

== Summary ==
Depending upon the nature of input and output of the function, the derivative takes appropriate forms. The following table summarizes the derivative structure for functions whose input and output are:

[cols="1,1,1,1"]
|===
|Input stem:[\downarrow] / Output stem:[\to] |Scalar |Vector |Matrix

|*Scalar* |Scalar |Vector |Matrix
|*Vector* |Row vector (transpose of the gradient vector) |Matrix (Jacobian matrix) |Higher order array
|*Matrix* |Matrix |Higher order array |Higher order array
|===

These represent the explicit notation of the derivative for different cases. An implicit view is to look at the derivative as a "linear operator" stem:[D f(\mathbf{X})] acting on perturbation stem:[d\mathbf{X}].

NOTE: Scalars are zero dimensional, vectors are 1 dimensional, matrices are two dimensional arrays, etc. The size(scalar) is [], size(vector) is stem:[[n\]], size of a matrix is stem:[[m,n\]], etc.

[bibliography]
== References ==

* Boyd, S. P., & Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.
* MIT OpenCourseWare. (n.d.). MIT OpenCourseWare. https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/resources/ocw_18s096_lecture01_part1_2023jan18_mp4/
* S2.2. (n.d.). https://www.math.toronto.edu/courses/mat237y1/20189/notes/Chapter2/S2.2.html
* 14.5 directional derivatives. (n.d.). https://www.whitman.edu/mathematics/calculus_online/section14.05.html
